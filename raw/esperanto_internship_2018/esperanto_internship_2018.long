Shortly about what we do @ Esperanto Tech. Think of the technical buzzword that you hear probably every day these days... Chances are you will guess what we do at the first attempt. That is correct: Machine Learning (ML) is what we do. Nevertheless, unlike the majority of all others, we are a hardware company. To be specific, we are a chip company where we build a thousand core chip that will accelerate computations required for ML applications.
<br><br>
Do you know why Machine Learning has become so popular recently? Despite the fact that this is a very mature field. It has decades of history. Thousands of papers have been published. The paper about today's super famous concept of "Neural Networks" was first published in the 80s. The main issue those days was the lack of computation resources. Neural Nets are ridiculously compute-hungry, and the time it took to compute was not realistic! That is why it just didn't seem so sexy. All of a sudden, the whole field just exploded! The credit for this explosion, I think, must be given to the emergence of General Purpose Graphics Processing Units (GPGPU). That was the emergence of the hardware capable of handling heavy computes in a reasonable period of time. As the name implies, GPGPU was initially designed to process graphics related data. But it just happened so that the architecture of GPUs perfectly mapped to ML applications, as well as many others including bitcoin mining :) As our CEO, Dave Ditzel explains machine learning and graphics application are often "embarrassingly parallel". In other words, these programs can be intelligently broken into thousands of smaller pieces and computed in parallel without jeopardizing the end result.
<br><br>
At Esperanto Technologies we are developing a chip that is better than a GPU when it comes to processing of ML programs. This chip will be faster and more power efficient. There is a rule of thumb in logic design:  "the more specific is your logic, the more efficient it is in terms of speed and power". So in short, what we do is: extremely careful characterization of Machine Learning workloads and the design of hardware support to tackle those specific characteristics.
